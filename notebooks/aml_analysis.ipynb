{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AML Feature Store - An√°lise e Demonstra√ß√£o\n",
        "\n",
        "Este notebook demonstra como usar o Feature Store para an√°lise de dados hist√≥ricos e treinamento de modelos de ML para detec√ß√£o de lavagem de dinheiro.\n",
        "\n",
        "## Objetivos\n",
        "1. Explorar dados hist√≥ricos usando o Feast Offline Store\n",
        "2. Analisar padr√µes de transa√ß√µes suspeitas\n",
        "3. Criar datasets de treinamento consistentes\n",
        "4. Treinar um modelo simples de detec√ß√£o de fraude\n",
        "5. Demonstrar a consist√™ncia entre ambiente offline e online\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports necess√°rios\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurar visualiza√ß√µes\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"üì¶ Bibliotecas importadas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports espec√≠ficos do projeto\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "from feast import FeatureStore\n",
        "import redis\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Inicializar Feature Store\n",
        "try:\n",
        "    store = FeatureStore(repo_path=\"../feature_repo\")\n",
        "    print(\"üéØ Feature Store inicializado!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erro ao inicializar Feature Store: {e}\")\n",
        "    store = None\n",
        "\n",
        "# Conectar ao Redis (Online Store)\n",
        "try:\n",
        "    redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
        "    redis_client.ping()\n",
        "    print(\"üîó Conectado ao Redis!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erro ao conectar ao Redis: {e}\")\n",
        "    redis_client = None\n",
        "\n",
        "print(\"üì¶ Bibliotecas avan√ßadas importadas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Gera√ß√£o e An√°lise Avan√ßada de Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gerar dados sint√©ticos avan√ßados se n√£o existirem\n",
        "import os\n",
        "if not os.path.exists('../offline_data/transactions.parquet'):\n",
        "    print(\"üìù Gerando dados sint√©ticos avan√ßados...\")\n",
        "    exec(open('../offline_data/generate_sample_data.py').read())\n",
        "else:\n",
        "    print(\"üìÅ Dados j√° existem, carregando...\")\n",
        "\n",
        "# Carregar dados hist√≥ricos\n",
        "df = pd.read_parquet('../offline_data/transactions.parquet')\n",
        "print(f\"üìà Dados carregados: {len(df):,} transa√ß√µes\")\n",
        "print(f\"üìÖ Per√≠odo: {df['event_timestamp'].min()} at√© {df['event_timestamp'].max()}\")\n",
        "print(f\"üë• Clientes √∫nicos: {df['customer_id'].nunique():,}\")\n",
        "print(f\"üè™ Estabelecimentos √∫nicos: {df['merchant_id'].nunique():,}\")\n",
        "\n",
        "# An√°lise de distribui√ß√£o temporal\n",
        "df['hour'] = pd.to_datetime(df['event_timestamp']).dt.hour\n",
        "df['day_of_week'] = pd.to_datetime(df['event_timestamp']).dt.dayofweek\n",
        "df['is_business_hours'] = df['hour'].between(9, 17)\n",
        "\n",
        "print(f\"\\nüìä Estat√≠sticas b√°sicas:\")\n",
        "print(f\"üí∞ Valor m√©dio: R$ {df['amount'].mean():.2f}\")\n",
        "print(f\"üìä Mediana: R$ {df['amount'].median():.2f}\")\n",
        "print(f\"üö® Taxa de suspeitas: {df['is_suspicious'].mean()*100:.1f}%\")\n",
        "print(f\"üåô Transa√ß√µes noturnas: {(df['hour'] < 6).sum() + (df['hour'] > 22).sum()} ({((df['hour'] < 6).sum() + (df['hour'] > 22).sum())/len(df)*100:.1f}%)\")\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Engenharia de Features Avan√ßada\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fun√ß√£o avan√ßada para calcular features temporais\n",
        "def calculate_advanced_features(df, customer_id, timestamp, windows):\n",
        "    \"\"\"Calcula features avan√ßadas de janela temporal com padr√µes comportamentais\"\"\"\n",
        "    \n",
        "    # Filtrar transa√ß√µes do cliente at√© o timestamp\n",
        "    customer_txns = df[\n",
        "        (df['customer_id'] == customer_id) & \n",
        "        (df['event_timestamp'] <= timestamp)\n",
        "    ].copy()\n",
        "    \n",
        "    features = {}\n",
        "    \n",
        "    for window_name, window_minutes in windows.items():\n",
        "        # Calcular cutoff time\n",
        "        cutoff_time = timestamp - timedelta(minutes=window_minutes)\n",
        "        \n",
        "        # Transa√ß√µes na janela\n",
        "        window_txns = customer_txns[customer_txns['event_timestamp'] > cutoff_time]\n",
        "        \n",
        "        if len(window_txns) == 0:\n",
        "            # Features b√°sicas zeradas\n",
        "            features.update({\n",
        "                f'txn_count_{window_name}': 0,\n",
        "                f'txn_amount_sum_{window_name}': 0.0,\n",
        "                f'avg_txn_amount_{window_name}': 0.0,\n",
        "                f'max_txn_amount_{window_name}': 0.0,\n",
        "                f'min_txn_amount_{window_name}': 0.0,\n",
        "                f'std_txn_amount_{window_name}': 0.0,\n",
        "                f'unique_merchants_{window_name}': 0,\n",
        "                f'unique_ips_{window_name}': 0,\n",
        "                f'velocity_score_{window_name}': 0.0,\n",
        "                f'night_txn_ratio_{window_name}': 0.0,\n",
        "                f'weekend_txn_ratio_{window_name}': 0.0,\n",
        "                f'business_hours_ratio_{window_name}': 0.0,\n",
        "                f'amount_concentration_gini_{window_name}': 0.0,\n",
        "                f'merchant_concentration_hhi_{window_name}': 0.0\n",
        "            })\n",
        "            continue\n",
        "        \n",
        "        # Features b√°sicas\n",
        "        amounts = window_txns['amount'].values\n",
        "        features[f'txn_count_{window_name}'] = len(window_txns)\n",
        "        features[f'txn_amount_sum_{window_name}'] = amounts.sum()\n",
        "        features[f'avg_txn_amount_{window_name}'] = amounts.mean()\n",
        "        features[f'max_txn_amount_{window_name}'] = amounts.max()\n",
        "        features[f'min_txn_amount_{window_name}'] = amounts.min()\n",
        "        features[f'std_txn_amount_{window_name}'] = amounts.std() if len(amounts) > 1 else 0.0\n",
        "        \n",
        "        # Features de diversidade\n",
        "        features[f'unique_merchants_{window_name}'] = window_txns['merchant_id'].nunique()\n",
        "        features[f'unique_ips_{window_name}'] = window_txns['ip_address'].nunique()\n",
        "        \n",
        "        # Features temporais\n",
        "        features[f'velocity_score_{window_name}'] = len(window_txns) / (window_minutes / 60.0)  # txn por hora\n",
        "        \n",
        "        # Ratios comportamentais\n",
        "        night_txns = window_txns[(window_txns['hour'] < 6) | (window_txns['hour'] > 22)]\n",
        "        features[f'night_txn_ratio_{window_name}'] = len(night_txns) / len(window_txns)\n",
        "        \n",
        "        weekend_txns = window_txns[window_txns['day_of_week'] >= 5]\n",
        "        features[f'weekend_txn_ratio_{window_name}'] = len(weekend_txns) / len(window_txns)\n",
        "        \n",
        "        business_txns = window_txns[window_txns['is_business_hours']]\n",
        "        features[f'business_hours_ratio_{window_name}'] = len(business_txns) / len(window_txns)\n",
        "        \n",
        "        # Features de concentra√ß√£o (indicadores de anomalia)\n",
        "        # Gini coefficient para distribui√ß√£o de valores\n",
        "        if len(amounts) > 1:\n",
        "            sorted_amounts = np.sort(amounts)\n",
        "            n = len(sorted_amounts)\n",
        "            cumsum = np.cumsum(sorted_amounts)\n",
        "            gini = (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n\n",
        "            features[f'amount_concentration_gini_{window_name}'] = gini\n",
        "        else:\n",
        "            features[f'amount_concentration_gini_{window_name}'] = 0.0\n",
        "        \n",
        "        # Herfindahl-Hirschman Index para concentra√ß√£o de estabelecimentos\n",
        "        merchant_counts = window_txns['merchant_id'].value_counts()\n",
        "        merchant_shares = merchant_counts / merchant_counts.sum()\n",
        "        hhi = (merchant_shares ** 2).sum()\n",
        "        features[f'merchant_concentration_hhi_{window_name}'] = hhi\n",
        "        \n",
        "        # Features de padr√£o sequencial\n",
        "        if len(window_txns) > 1:\n",
        "            # Tempo m√©dio entre transa√ß√µes\n",
        "            time_diffs = window_txns['event_timestamp'].diff().dt.total_seconds().dropna()\n",
        "            if len(time_diffs) > 0:\n",
        "                features[f'avg_time_between_txns_{window_name}'] = time_diffs.mean()\n",
        "                features[f'std_time_between_txns_{window_name}'] = time_diffs.std()\n",
        "            else:\n",
        "                features[f'avg_time_between_txns_{window_name}'] = 0.0\n",
        "                features[f'std_time_between_txns_{window_name}'] = 0.0\n",
        "        else:\n",
        "            features[f'avg_time_between_txns_{window_name}'] = 0.0\n",
        "            features[f'std_time_between_txns_{window_name}'] = 0.0\n",
        "    \n",
        "    return features\n",
        "\n",
        "# Definir janelas temporais mais granulares\n",
        "time_windows = {\n",
        "    '1m': 1,\n",
        "    '5m': 5, \n",
        "    '15m': 15,\n",
        "    '1h': 60,\n",
        "    '6h': 360,\n",
        "    '24h': 1440\n",
        "}\n",
        "\n",
        "print(\"üîß Fun√ß√£o de engenharia de features avan√ßada criada!\")\n",
        "print(f\"üìä Janelas temporais: {list(time_windows.keys())}\")\n",
        "print(f\"üéØ Features por janela: ~15 features √ó {len(time_windows)} janelas = ~{15 * len(time_windows)} features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Modelos Avan√ßados de Machine Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar dataset avan√ßado com features engineered\n",
        "print(\"üîÑ Criando dataset avan√ßado com features engineered...\")\n",
        "print(\"‚ö†Ô∏è Isso pode levar alguns minutos para processar...\")\n",
        "\n",
        "# Usar uma amostra maior para modelos mais robustos\n",
        "sample_size = 10000\n",
        "df_sample = df.sample(n=min(sample_size, len(df)), random_state=42).sort_values('event_timestamp')\n",
        "\n",
        "advanced_training_data = []\n",
        "\n",
        "for idx, row in df_sample.iterrows():\n",
        "    if idx % 2000 == 0:\n",
        "        print(f\"   Processando transa√ß√£o {idx}/{len(df_sample)}...\")\n",
        "    \n",
        "    # Calcular features avan√ßadas para esta transa√ß√£o\n",
        "    features = calculate_advanced_features(\n",
        "        df, row['customer_id'], row['event_timestamp'], time_windows\n",
        "    )\n",
        "    \n",
        "    # Adicionar informa√ß√µes da transa√ß√£o atual\n",
        "    record = {\n",
        "        'transaction_id': row['transaction_id'],\n",
        "        'customer_id': row['customer_id'],\n",
        "        'merchant_id': row['merchant_id'],\n",
        "        'amount': row['amount'],\n",
        "        'hour': row['hour'],\n",
        "        'day_of_week': row['day_of_week'],\n",
        "        'is_weekend': row['is_weekend'],\n",
        "        'is_business_hours': row['is_business_hours'],\n",
        "        'event_timestamp': row['event_timestamp'],\n",
        "        'is_suspicious': row['is_suspicious'],  # Target variable\n",
        "        **features\n",
        "    }\n",
        "    \n",
        "    advanced_training_data.append(record)\n",
        "\n",
        "# Converter para DataFrame\n",
        "training_df_advanced = pd.DataFrame(advanced_training_data)\n",
        "\n",
        "print(f\"‚úÖ Dataset avan√ßado criado: {len(training_df_advanced)} amostras\")\n",
        "print(f\"üìä Total de features: {len(training_df_advanced.columns) - 6}\")  # Excluir colunas n√£o-feature\n",
        "\n",
        "# Identificar colunas de features\n",
        "feature_columns = [col for col in training_df_advanced.columns \n",
        "                  if col not in ['transaction_id', 'customer_id', 'merchant_id', \n",
        "                               'event_timestamp', 'is_suspicious']]\n",
        "\n",
        "print(f\"üéØ Features para modelagem: {len(feature_columns)}\")\n",
        "\n",
        "# Mostrar algumas estat√≠sticas\n",
        "print(f\"\\nüìà Distribui√ß√£o do target:\")\n",
        "print(training_df_advanced['is_suspicious'].value_counts())\n",
        "print(f\"Taxa de desbalanceamento: {training_df_advanced['is_suspicious'].value_counts()[0] / training_df_advanced['is_suspicious'].value_counts()[1]:.1f}:1\")\n",
        "\n",
        "training_df_advanced.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar dados para modelagem avan√ßada\n",
        "print(\"üîÑ Preparando dados para modelagem...\")\n",
        "\n",
        "# Selecionar features e target\n",
        "X = training_df_advanced[feature_columns].fillna(0)  # Preencher NaN com 0\n",
        "y = training_df_advanced['is_suspicious']\n",
        "\n",
        "print(f\"üìä Shape dos dados: {X.shape}\")\n",
        "print(f\"üéØ Distribui√ß√£o do target: {y.value_counts().to_dict()}\")\n",
        "\n",
        "# An√°lise de correla√ß√£o avan√ßada\n",
        "correlation_matrix = X.corrwith(y).sort_values(key=abs, ascending=False)\n",
        "print(f\"\\nüîç Top 10 features mais correlacionadas:\")\n",
        "for feature, corr in correlation_matrix.head(10).items():\n",
        "    print(f\"  {feature}: {corr:.4f}\")\n",
        "\n",
        "# Visualizar correla√ß√µes\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = correlation_matrix.head(15)\n",
        "colors = ['red' if x > 0 else 'blue' for x in top_features.values]\n",
        "bars = plt.barh(range(len(top_features)), top_features.values, color=colors, alpha=0.7)\n",
        "plt.yticks(range(len(top_features)), top_features.index, fontsize=10)\n",
        "plt.xlabel('Correla√ß√£o com Target (is_suspicious)')\n",
        "plt.title('Top 15 Features Mais Correlacionadas')\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "\n",
        "for i, (bar, value) in enumerate(zip(bars, top_features.values)):\n",
        "    plt.text(value + 0.01 if value > 0 else value - 0.01, i, f'{value:.3f}', \n",
        "             va='center', ha='left' if value > 0 else 'right', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Treinamento de m√∫ltiplos modelos avan√ßados\n",
        "print(\"ü§ñ Treinando m√∫ltiplos modelos de ML...\")\n",
        "\n",
        "# Split dos dados\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"üìä Treino: {X_train.shape[0]} amostras\")\n",
        "print(f\"üìä Teste: {X_test.shape[0]} amostras\")\n",
        "\n",
        "# Definir modelos para compara√ß√£o\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=100, max_depth=10, min_samples_split=5,\n",
        "        random_state=42, class_weight='balanced'\n",
        "    ),\n",
        "    'XGBoost': xgb.XGBClassifier(\n",
        "        n_estimators=100, max_depth=6, learning_rate=0.1,\n",
        "        random_state=42, scale_pos_weight=10  # Para desbalanceamento\n",
        "    ),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(\n",
        "        n_estimators=100, max_depth=6, learning_rate=0.1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        random_state=42, class_weight='balanced', max_iter=1000\n",
        "    )\n",
        "}\n",
        "\n",
        "# Treinar e avaliar modelos\n",
        "results = {}\n",
        "trained_models = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nüîÑ Treinando {name}...\")\n",
        "    \n",
        "    # Treinar modelo\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Fazer predi√ß√µes\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calcular m√©tricas\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "    \n",
        "    results[name] = {\n",
        "        'auc_score': auc_score,\n",
        "        'training_time': training_time,\n",
        "        'predictions': y_pred,\n",
        "        'probabilities': y_pred_proba\n",
        "    }\n",
        "    \n",
        "    trained_models[name] = model\n",
        "    \n",
        "    print(f\"  ‚úÖ AUC: {auc_score:.4f} | Tempo: {training_time:.2f}s\")\n",
        "\n",
        "# Mostrar compara√ß√£o de modelos\n",
        "print(f\"\\nüìä COMPARA√á√ÉO DE MODELOS:\")\n",
        "print(\"=\" * 50)\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name:20} | AUC: {metrics['auc_score']:.4f} | Tempo: {metrics['training_time']:.2f}s\")\n",
        "\n",
        "# Identificar melhor modelo\n",
        "best_model_name = max(results.keys(), key=lambda x: results[x]['auc_score'])\n",
        "best_model = trained_models[best_model_name]\n",
        "print(f\"\\nüèÜ Melhor modelo: {best_model_name} (AUC: {results[best_model_name]['auc_score']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. An√°lise Detalhada do Melhor Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise detalhada do melhor modelo\n",
        "print(f\"üîç Analisando modelo: {best_model_name}\")\n",
        "\n",
        "# Obter predi√ß√µes do melhor modelo\n",
        "best_predictions = results[best_model_name]['predictions']\n",
        "best_probabilities = results[best_model_name]['probabilities']\n",
        "\n",
        "# Visualiza√ß√µes avan√ßadas\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Confusion Matrix\n",
        "cm = confusion_matrix(y_test, best_predictions)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
        "axes[0,0].set_title(f'Matriz de Confus√£o - {best_model_name}')\n",
        "axes[0,0].set_xlabel('Predito')\n",
        "axes[0,0].set_ylabel('Real')\n",
        "\n",
        "# 2. ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, best_probabilities)\n",
        "auc = results[best_model_name]['auc_score']\n",
        "axes[0,1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {auc:.3f})')\n",
        "axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[0,1].set_xlim([0.0, 1.0])\n",
        "axes[0,1].set_ylim([0.0, 1.05])\n",
        "axes[0,1].set_xlabel('Taxa de Falsos Positivos')\n",
        "axes[0,1].set_ylabel('Taxa de Verdadeiros Positivos')\n",
        "axes[0,1].set_title('Curva ROC')\n",
        "axes[0,1].legend(loc=\"lower right\")\n",
        "axes[0,1].grid(alpha=0.3)\n",
        "\n",
        "# 3. Precision-Recall Curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, best_probabilities)\n",
        "axes[1,0].plot(recall, precision, color='purple', lw=2)\n",
        "axes[1,0].set_xlabel('Recall')\n",
        "axes[1,0].set_ylabel('Precision')\n",
        "axes[1,0].set_title('Curva Precision-Recall')\n",
        "axes[1,0].grid(alpha=0.3)\n",
        "\n",
        "# 4. Feature Importance (se dispon√≠vel)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_columns,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False).head(15)\n",
        "    \n",
        "    axes[1,1].barh(range(len(feature_importance)), feature_importance['importance'], color='lightgreen')\n",
        "    axes[1,1].set_yticks(range(len(feature_importance)))\n",
        "    axes[1,1].set_yticklabels(feature_importance['feature'], fontsize=8)\n",
        "    axes[1,1].set_xlabel('Import√¢ncia')\n",
        "    axes[1,1].set_title('Top 15 Features Mais Importantes')\n",
        "    axes[1,1].grid(axis='x', alpha=0.3)\n",
        "else:\n",
        "    axes[1,1].text(0.5, 0.5, 'Feature importance\\nn√£o dispon√≠vel\\npara este modelo', \n",
        "                   ha='center', va='center', transform=axes[1,1].transAxes)\n",
        "    axes[1,1].set_title('Feature Importance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Relat√≥rio detalhado\n",
        "print(f\"\\nüìä RELAT√ìRIO DETALHADO - {best_model_name}\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, best_predictions, target_names=['Normal', 'Suspeita']))\n",
        "\n",
        "# An√°lise de threshold\n",
        "thresholds = np.arange(0.1, 1.0, 0.1)\n",
        "threshold_metrics = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = (best_probabilities >= threshold).astype(int)\n",
        "    \n",
        "    if len(np.unique(y_pred_thresh)) > 1:  # Evitar divis√£o por zero\n",
        "        precision = precision_score(y_test, y_pred_thresh)\n",
        "        recall = recall_score(y_test, y_pred_thresh)\n",
        "        f1 = f1_score(y_test, y_pred_thresh)\n",
        "    else:\n",
        "        precision = recall = f1 = 0\n",
        "    \n",
        "    threshold_metrics.append({\n",
        "        'threshold': threshold,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    })\n",
        "\n",
        "threshold_df = pd.DataFrame(threshold_metrics)\n",
        "print(f\"\\nüéØ AN√ÅLISE DE THRESHOLD:\")\n",
        "print(threshold_df.round(3))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
